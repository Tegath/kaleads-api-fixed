# Guide Complet : Scraping Ã‰conomique de Toutes les Villes

## ğŸ¯ Vue d'ensemble

Le systÃ¨me a Ã©tÃ© modifiÃ© pour scraper **TOUTES les villes de France et Wallonie** de maniÃ¨re Ã©conomique avec :
- âœ… **Pagination intelligente** : s'arrÃªte automatiquement quand plus de rÃ©sultats
- âœ… **DÃ©duplication automatique** : Ã©vite les doublons en base Supabase
- âœ… **Stockage persistant** : tous les leads sont sauvegardÃ©s pour rÃ©utilisation
- âœ… **Mode comprehensive** : scraping complet one-time pour Google Maps
- âœ… **Mode journalier/hebdomadaire** : pour JobSpy (offres d'emploi)

---

## ğŸ“Š Statistiques

### Villes disponibles
- **France** : ~35 000 villes
- **Wallonie** : ~262 villes
- **Total** : ~35 262 villes

### Estimation pour une recherche complÃ¨te
- **Query** : "agence marketing"
- **Villes** : 35 262
- **RÃ©sultats estimÃ©s par ville** : 10-30 (moyenne 20)
- **Total leads estimÃ©s** : ~700 000 leads
- **Temps estimÃ©** : 245 heures (~10 jours en background)
- **CoÃ»t RapidAPI estimÃ©** : ~$350 (Ã  $0.001/page, ~350k pages)

---

## ğŸ› ï¸ Ã‰tape 1 : CrÃ©er la table Supabase

### 1.1 AccÃ©der Ã  Supabase SQL Editor
1. Aller sur https://supabase.com/dashboard
2. SÃ©lectionner votre projet
3. Aller dans **SQL Editor**

### 1.2 ExÃ©cuter le script SQL
Copier-coller le contenu de [`supabase_leads_table.sql`](supabase_leads_table.sql) et exÃ©cuter.

Cela va crÃ©er :
- âœ… Table `leads` avec dÃ©duplication (lead_hash unique)
- âœ… Index pour requÃªtes rapides
- âœ… RLS policies
- âœ… Trigger auto-update `updated_at`

### 1.3 VÃ©rifier la crÃ©ation
```sql
SELECT COUNT(*) FROM leads; -- Devrait retourner 0 (table vide)
```

---

## ğŸš€ Ã‰tape 2 : Workflow n8n - Mode Comprehensive

### Architecture du workflow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Coordinator  â”‚ â†’ GÃ©nÃ¨re stratÃ©gie avec ALL_CITIES
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. Google Maps  â”‚ â†’ Scrape TOUTES les villes + pagination intelligente
â”‚    Comprehensiveâ”‚ â†’ Stocke en Supabase avec dÃ©duplication
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. JobSpy       â”‚ â†’ Recherche offres d'emploi (journalier/hebdomadaire)
â”‚    (Optional)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. Email Gen    â”‚ â†’ GÃ©nÃ¨re emails pour les leads qualifiÃ©s
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Node 1 : HTTP Request - Coordinator

**Settings:**
- **URL** : `http://kaleads-atomic-agents:20001/api/v2/coordinator/analyze`
- **Method** : POST
- **Headers** : `X-API-Key: lL^nc2U%tU8f2!LH48!29!mW8`
- **Body JSON** :
```json
{
  "client_id": "kaleads",
  "target_count": 500000,
  "country": "France"
}
```

**Response attendue** :
```json
{
  "pain_type": "lead_generation",
  "strategy": "hybrid",
  "google_maps_searches": [
    {
      "query": "agence marketing digital",
      "cities": "ALL_CITIES",
      "country": "France",
      "use_pagination": true,
      "comprehensive": true
    },
    // ... 4 autres queries
  ],
  "execution_plan": {
    "mode": "COMPREHENSIVE_SCRAPING",
    "cities_count": 35262,
    "estimated_time": "245 hours (background process)"
  }
}
```

---

### Node 2 : Split In Batches - Google Maps Searches

**Settings:**
- **Batch Size** : 1
- **Input Field** : `google_maps_searches`

---

### Node 3 : HTTP Request - Execute Google Maps (Comprehensive)

**IMPORTANT** : Ce call va prendre des HEURES Ã  complÃ©ter. Il faut le lancer en mode background.

**Settings:**
- **URL** : `http://kaleads-atomic-agents:20001/api/v2/leads/google-maps`
- **Method** : POST
- **Headers** : `X-API-Key: lL^nc2U%tU8f2!LH48!29!mW8`
- **Timeout** : 86400000 (24 heures en ms)
- **Body JSON** :
```json
{
  "query": "{{ $json.query }}",
  "cities": "{{ $json.cities }}",
  "country": "{{ $json.country }}",
  "use_pagination": {{ $json.use_pagination }},
  "comprehensive": {{ $json.comprehensive }},
  "client_id": "kaleads"
}
```

**Ce que fait ce call** :
1. Charge TOUTES les villes depuis les CSV
2. Pour chaque ville :
   - Lance une recherche Google Maps
   - Pagination intelligente (continue jusqu'Ã  plus de rÃ©sultats)
   - DÃ©duplication en mÃ©moire
3. Stocke TOUS les leads en Supabase
4. Retourne les statistiques

**Response attendue** :
```json
{
  "success": true,
  "leads": [...],  // Peut Ãªtre vide si comprehensive=true (stockÃ© en DB)
  "total_leads": 123456,
  "cities_searched": ["ALL_CITIES"],
  "cost_usd": 12.34
}
```

---

### Node 4 : Alternative - Mode Background (RecommandÃ©)

**ProblÃ¨me** : n8n va timeout aprÃ¨s 24h mÃªme avec timeout max.

**Solution** : CrÃ©er un endpoint sÃ©parÃ© qui lance le scraping en background.

#### Option A : Webhook Trigger + Background Task

1. CrÃ©er un webhook n8n qui dÃ©clenche le scraping
2. Le scraping se fait en background sur le serveur
3. n8n reÃ§oit immÃ©diatement une rÃ©ponse "Job started"
4. Le scraping continue pendant des jours si nÃ©cessaire
5. Les rÃ©sultats sont stockÃ©s en Supabase au fur et Ã  mesure

#### Option B : Cron Job quotidien

Lancer le scraping par batches :
- Jour 1 : Villes A-D (7000 villes)
- Jour 2 : Villes E-L (7000 villes)
- Jour 3 : Villes M-R (7000 villes)
- Etc.

---

## ğŸ“ˆ Ã‰tape 3 : Monitoring et Queries Supabase

### Compter les leads scrapÃ©s en temps rÃ©el

```sql
SELECT
  source,
  COUNT(*) as total_leads,
  COUNT(DISTINCT city) as cities_covered
FROM leads
WHERE client_id = 'kaleads'
GROUP BY source;
```

### Top 10 villes avec le plus de leads

```sql
SELECT
  city,
  COUNT(*) as lead_count
FROM leads
WHERE client_id = 'kaleads' AND source = 'google_maps'
GROUP BY city
ORDER BY lead_count DESC
LIMIT 10;
```

### Progression du scraping (nouveaux leads par heure)

```sql
SELECT
  DATE_TRUNC('hour', created_at) as hour,
  COUNT(*) as leads_added
FROM leads
WHERE client_id = 'kaleads'
GROUP BY hour
ORDER BY hour DESC
LIMIT 24;
```

---

## ğŸ’° Optimisation des CoÃ»ts

### StratÃ©gie 1 : Filtrer les villes par taille
Ne scraper que les villes > 1000 habitants pour Ã©conomiser ~90% des calls.

**Modifier** [`cities_loader.py`](src/helpers/cities_loader.py) pour filtrer :
```python
def get_all_cities(self, country: str = "France", min_population: int = 1000) -> List[str]:
    # Filtrer par population si CSV a cette info
    pass
```

### StratÃ©gie 2 : Limiter les pages par ville
Au lieu de scraper TOUTES les pages, limiter Ã  5 pages max par ville.

**Modifier** dans [`google_maps_integration.py`](kaleads-atomic-agents/src/integrations/google_maps_integration.py:214) :
```python
# Safety: max 5 pages per city for cost control
if page > 5:
    logger.warning(f"Reached max page limit (5) for {city}")
    break
```

### StratÃ©gie 3 : Scraping intelligent par prioritÃ©
1. Scraper d'abord les **grandes villes** (Paris, Lyon, etc.)
2. Analyser le taux de conversion
3. Si bon ROI â†’ continuer avec petites villes
4. Si mauvais ROI â†’ arrÃªter

---

## ğŸ”„ Mode Journalier/Hebdomadaire pour JobSpy

JobSpy devrait Ãªtre lancÃ© rÃ©guliÃ¨rement car les offres d'emploi changent souvent.

### Workflow sÃ©parÃ© : JobSpy Daily Refresh

**Cron** : Tous les jours Ã  6h du matin
**Node 1** : HTTP Request - Coordinator (mÃªme qu'avant)
**Node 2** : Split In Batches - JobSpy Searches
**Node 3** : HTTP Request - Execute JobSpy
```json
{
  "job_title": "{{ $json.job_title }}",
  "location": "{{ $json.location }}",
  "company_size": {{ $json.company_size }},
  "industries": {{ $json.industries }},
  "max_results": {{ $json.max_results }}
}
```

---

## ğŸ› Debugging et Logs

### Voir les logs du scraping en temps rÃ©el

```bash
# SSH sur le serveur
ssh root@92.112.193.183

# Voir les logs Docker
docker logs -f kaleads-atomic-agents

# Filtrer pour voir uniquement le scraping
docker logs -f kaleads-atomic-agents 2>&1 | grep "google_maps"
```

### VÃ©rifier l'Ã©tat du scraping

```bash
# Compter les leads en DB
curl -X POST https://your-supabase.supabase.co/rest/v1/rpc/count_leads \
  -H "apikey: YOUR_SUPABASE_ANON_KEY" \
  -H "Authorization: Bearer YOUR_SUPABASE_ANON_KEY"
```

---

## ğŸ“ Checklist avant de lancer le scraping complet

- [ ] Table `leads` crÃ©Ã©e dans Supabase
- [ ] RLS policies configurÃ©es
- [ ] Index crÃ©Ã©s pour performance
- [ ] CSV des villes prÃ©sents dans le projet
- [ ] RapidAPI key configurÃ©e dans .env
- [ ] Budget RapidAPI vÃ©rifiÃ© (~$350 nÃ©cessaire)
- [ ] Workflow n8n testÃ© avec 3 villes
- [ ] Logs monitoring configurÃ©s
- [ ] Plan de backup des donnÃ©es Supabase

---

## ğŸ¯ Exemple : Workflow n8n SimplifiÃ©

Pour Ã©viter la complexitÃ©, voici un workflow minimaliste :

### Option Simple : Script Python Background

Au lieu d'utiliser n8n pour un scraping de plusieurs jours, crÃ©er un script Python standalone :

```python
# comprehensive_scraper.py
from src.integrations.google_maps_integration import GoogleMapsLeadGenerator
from src.providers.leads_storage import LeadsStorage

gmaps = GoogleMapsLeadGenerator()
storage = LeadsStorage(client_id="kaleads")

# Scrape comprehensive
queries = [
    "agence marketing digital",
    "startup SaaS",
    "agence web",
    "Ã©diteur de logiciel",
    "entreprise technologique"
]

for query in queries:
    print(f"Scraping: {query}")
    leads = gmaps.search_all_cities_comprehensive(
        query=query,
        country="France"
    )

    stats = storage.store_leads(leads)
    print(f"âœ… {query}: {stats}")
```

**Lancer en background** :
```bash
nohup python comprehensive_scraper.py > scraping.log 2>&1 &
```

**Avantages** :
- âœ… Pas de timeout n8n
- âœ… Logs clairs
- âœ… Peut tourner pendant des jours
- âœ… RÃ©sultats en Supabase accessibles immÃ©diatement

---

## ğŸ“ Support

Si des erreurs surviennent :
1. VÃ©rifier les logs Docker
2. VÃ©rifier Supabase (table accessible ?)
3. VÃ©rifier RapidAPI (quota restant ?)
4. Tester avec 1 ville d'abord

**Bon scraping! ğŸš€**
