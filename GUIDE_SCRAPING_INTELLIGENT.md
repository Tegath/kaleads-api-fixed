# ğŸš€ Guide : Scraping Intelligent avec Background Jobs

## ğŸ¯ Le ProblÃ¨me RÃ©solu

Avant, tu devais :
- âŒ GÃ©rer manuellement la liste des villes
- âŒ Risquer de perdre toutes les donnÃ©es en cas de timeout
- âŒ Payer pour scraper des petites villes peu rentables
- âŒ Attendre que tout soit fini avant d'avoir des rÃ©sultats

Maintenant, le systÃ¨me :
- âœ… **Choisit automatiquement** les villes Ã  scraper selon leur population
- âœ… **Sauvegarde les leads au fur et Ã  mesure** (toutes les 100 leads)
- âœ… **Continue en background** mÃªme si tu fermes n8n
- âœ… **Peut reprendre** oÃ¹ il s'est arrÃªtÃ© en cas d'interruption
- âœ… **Te montre la progression en temps rÃ©el**

---

## ğŸ“Š StratÃ©gie Intelligente par Population

### Niveaux de PrioritÃ©

| PrioritÃ© | Population | Max Pages | StratÃ©gie | Exemple |
|----------|------------|-----------|-----------|---------|
| **1 - HIGH** | > 100 000 hab | 10 pages | Comprehensive | Paris, Lyon, Marseille |
| **2 - MEDIUM** | 20k - 100k | 5 pages | Moderate | Angers, Brest, Reims |
| **3 - LOW** | 5k - 20k | 2 pages | Light | VitrÃ©, Concarneau |
| **SKIP** | < 5 000 hab | 0 pages | Skip | Petits villages |

### Exemples de CoÃ»ts

**Scraping "agence marketing" avec max_priority=1 (seulement >100k hab)**
- Villes : ~150
- Leads estimÃ©s : ~22 500
- CoÃ»t : ~$11
- Temps : ~25 minutes

**Scraping "agence marketing" avec max_priority=2 (>20k hab)**
- Villes : ~900
- Leads estimÃ©s : ~67 500
- CoÃ»t : ~$34
- Temps : ~90 minutes

**Scraping "agence marketing" avec max_priority=3 (>5k hab)**
- Villes : ~3 500
- Leads estimÃ©s : ~122 000
- CoÃ»t : ~$61
- Temps : ~170 minutes

---

## ğŸ› ï¸ Utilisation Simple

### MÃ©thode 1 : Call HTTP Direct (RecommandÃ©)

#### Lancer un scraping

```http
POST http://92.112.193.183:20001/api/v2/scraping/start
Headers: X-API-Key: lL^nc2U%tU8f2!LH48!29!mW8

Params:
?query=agence marketing
&max_priority=2
&country=France
```

**Response :**
```json
{
  "job_id": "abc-123-def",
  "status": "pending",
  "estimated_cities": 900,
  "estimated_cost_usd": 34.50,
  "message": "Job started. Monitor at /api/v2/scraping/status/abc-123-def"
}
```

#### Voir la progression

```http
GET http://92.112.193.183:20001/api/v2/scraping/status/abc-123-def
Headers: X-API-Key: lL^nc2U%tU8f2!LH48!29!mW8
```

**Response :**
```json
{
  "job_id": "abc-123-def",
  "status": "running",
  "progress_pct": 45.5,
  "cities_completed": 410,
  "total_cities": 900,
  "total_leads_found": 28 450,
  "current_city": "Lyon",
  "estimated_cost_usd": 34.50
}
```

#### Voir tous les jobs

```http
GET http://92.112.193.183:20001/api/v2/scraping/jobs?status=running
Headers: X-API-Key: lL^nc2U%tU8f2!LH48!29!mW8
```

**Response :**
```json
{
  "jobs": [
    {
      "job_id": "abc-123",
      "query": "agence marketing",
      "status": "running",
      "progress_pct": 45.5,
      "total_leads_found": 28450
    },
    {
      "job_id": "def-456",
      "query": "startup SaaS",
      "status": "completed",
      "progress_pct": 100,
      "total_leads_found": 12890
    }
  ]
}
```

---

## ğŸ”„ Reprendre un Job Interrompu

Si le serveur plante, le Docker redÃ©marre, ou que tu stoppes un job :

```http
POST http://92.112.193.183:20001/api/v2/scraping/resume/abc-123-def
Headers: X-API-Key: lL^nc2U%tU8f2!LH48!29!mW8
```

Le job reprendra **exactement oÃ¹ il s'Ã©tait arrÃªtÃ©** !

---

## ğŸ“– Workflow n8n RecommandÃ©

### Node 1 : Start Scraping Job

**Type** : HTTP Request
**URL** : `http://kaleads-atomic-agents:20001/api/v2/scraping/start`
**Method** : POST
**Headers** : `X-API-Key: lL^nc2U%tU8f2!LH48!29!mW8`
**Query Params** :
```
query: {{ $json.search_query }}
max_priority: 2
country: France
```

### Node 2 : Wait 5 seconds

**Type** : Wait
**Time** : 5 seconds

### Node 3 : Check Job Status (Loop)

**Type** : HTTP Request
**URL** : `http://kaleads-atomic-agents:20001/api/v2/scraping/status/{{ $json.job_id }}`
**Method** : GET
**Headers** : `X-API-Key: lL^nc2U%tU8f2!LH48!29!mW8`

### Node 4 : IF Status == "completed"

**Type** : IF
**Condition** : `{{ $json.status }} == "completed"`

- **TRUE** â†’ Continue workflow (load leads from Supabase)
- **FALSE** â†’ Wait 30s and loop back to Node 3

### Node 5 : Load Leads from Supabase

**Type** : Supabase Query
**Table** : `leads`
**Filter** : `source = 'google_maps' AND client_id = 'kaleads'`
**Order** : `created_at DESC`
**Limit** : 1000

### Node 6 : Email Generation

Feed les leads Ã  ton systÃ¨me d'email generation !

---

## âš™ï¸ Configuration AvancÃ©e

### ParamÃ¨tres Disponibles

| ParamÃ¨tre | Description | DÃ©faut | Valeurs |
|-----------|-------------|--------|---------|
| `query` | RequÃªte de recherche | Required | "agence marketing" |
| `country` | Pays | "France" | "France", "Wallonie" |
| `min_population` | Pop minimale | 5000 | 1000-50000 |
| `max_priority` | PrioritÃ© max | 3 | 1 (HIGH), 2 (MEDIUM), 3 (ALL) |
| `client_id` | ID client Supabase | "kaleads" | Any string |

### Exemples d'Usage

**Scraping rapide (grandes villes uniquement)**
```
?query=agence+web&max_priority=1
â†’ ~150 villes, ~$11, ~25 min
```

**Scraping complet (toutes villes >5k hab)**
```
?query=agence+marketing&max_priority=3
â†’ ~3500 villes, ~$61, ~170 min
```

**Scraping Wallonie**
```
?query=startup+tech&country=Wallonie&max_priority=3
â†’ ~100 villes, ~$5, ~15 min
```

**Scraping ultra-sÃ©lectif (>50k hab)**
```
?query=cabinet+avocat&min_population=50000&max_priority=2
â†’ ~80 villes, ~$8, ~18 min
```

---

## ğŸ¯ Monitoring en Temps RÃ©el

### Query Supabase pour voir les jobs actifs

```sql
SELECT
  id,
  query,
  status,
  ROUND((cities_completed::float / total_cities::float) * 100, 2) as progress_pct,
  total_leads_found,
  current_city,
  estimated_cost_usd,
  created_at
FROM scraping_jobs
WHERE status IN ('pending', 'running')
ORDER BY created_at DESC;
```

### Query pour voir les leads du dernier scraping

```sql
SELECT
  company_name,
  city,
  phone,
  website,
  created_at
FROM leads
WHERE client_id = 'kaleads'
  AND source = 'google_maps'
  AND created_at > NOW() - INTERVAL '1 hour'
ORDER BY created_at DESC
LIMIT 100;
```

### Query pour statistiques par ville

```sql
SELECT
  city,
  COUNT(*) as lead_count,
  COUNT(DISTINCT company_name) as unique_companies
FROM leads
WHERE client_id = 'kaleads'
  AND source = 'google_maps'
GROUP BY city
ORDER BY lead_count DESC
LIMIT 20;
```

---

## ğŸ”§ Setup Initial (Une Fois)

### Ã‰tape 1 : CrÃ©er les tables Supabase

ExÃ©cuter ces 2 scripts SQL dans Supabase :

1. [supabase_leads_table.sql](supabase_leads_table.sql)
   - Table `leads` pour stocker tous les leads
2. [supabase_scraping_jobs_table.sql](supabase_scraping_jobs_table.sql)
   - Table `scraping_jobs` pour tracker les jobs
   - Table `city_strategy` pour la stratÃ©gie par ville

### Ã‰tape 2 : VÃ©rifier les CSV

Ces fichiers doivent Ãªtre dans le Docker :
- âœ… `Villes_france.csv` (~36k villes)
- âœ… `Villes_belgique.csv` (~262 villes)
- âœ… `Population_villes_france.csv` (donnÃ©es de population)

VÃ©rifier :
```bash
docker exec kaleads-atomic-agents ls -lh *.csv
```

### Ã‰tape 3 : Tester avec une petite query

```bash
curl -X POST "http://92.112.193.183:20001/api/v2/scraping/start?query=test&max_priority=1" \
  -H "X-API-Key: lL^nc2U%tU8f2!LH48!29!mW8"
```

---

## ğŸ“ˆ Optimisations RecommandÃ©es

### 1. Scraper par Tranches

Au lieu de tout scraper d'un coup, scrape par prioritÃ© :

**Jour 1** : Scrape HIGH (max_priority=1)
```
POST /api/v2/scraping/start?query=agence+marketing&max_priority=1
â†’ ~150 villes, ~$11, ~25 min
```

**Jour 2** : Si besoin de plus, scrape MEDIUM (max_priority=2)
```
POST /api/v2/scraping/start?query=agence+marketing&max_priority=2
â†’ ~900 villes, ~$34, ~90 min
```

**Jour 3** : Si toujours besoin, scrape ALL (max_priority=3)
```
POST /api/v2/scraping/start?query=agence+marketing&max_priority=3
â†’ ~3500 villes, ~$61, ~170 min
```

### 2. Analyser le ROI Avant de Continuer

AprÃ¨s chaque tranche, vÃ©rifie le taux de conversion :

```sql
SELECT
  CASE
    WHEN population >= 100000 THEN 'HIGH'
    WHEN population >= 20000 THEN 'MEDIUM'
    ELSE 'LOW'
  END as priority,
  COUNT(*) as total_leads,
  COUNT(*) FILTER (WHERE email IS NOT NULL) as leads_with_email,
  ROUND(COUNT(*) FILTER (WHERE email IS NOT NULL)::float / COUNT(*)::float * 100, 2) as email_pct
FROM leads
LEFT JOIN city_strategy USING (city_name)
WHERE client_id = 'kaleads'
GROUP BY 1
ORDER BY 1;
```

Si le priority LOW a un mauvais ROI â†’ arrÃªte lÃ , pas besoin de scraper les petites villes !

---

## ğŸ› Troubleshooting

### Job bloquÃ© en "running"

```sql
-- Voir le dernier checkpoint
SELECT last_checkpoint, current_city, updated_at
FROM scraping_jobs
WHERE id = 'job-id-here';

-- Si bloquÃ© depuis >1h, reprendre
```

```http
POST /api/v2/scraping/resume/job-id-here
```

### Pas de leads sauvegardÃ©s

VÃ©rifier les RLS policies dans Supabase :
```sql
-- DÃ©sactiver temporairement pour debug
ALTER TABLE leads DISABLE ROW LEVEL SECURITY;
```

### Job failed avec erreur "RapidAPI"

VÃ©rifier le quota RapidAPI :
- Aller sur https://rapidapi.com/dashboard
- VÃ©rifier les calls restants
- Augmenter le plan si nÃ©cessaire

---

## ğŸ’¡ Cas d'Usage

### Cas 1 : Prospection Rapide (1 Query)

**Objectif** : 1000 leads de qualitÃ© rapidement

```http
POST /api/v2/scraping/start
?query=agence+marketing
&max_priority=1
&min_population=50000
```

â†’ ~80 villes, ~$8, ~18 minutes, ~1200 leads

### Cas 2 : Base de DonnÃ©es ComplÃ¨te (Multiple Queries)

**Objectif** : Base de 50k+ leads pour campagnes longue durÃ©e

Lancer 5 jobs en parallÃ¨le :
```
Job 1: "agence marketing digital" - max_priority=2
Job 2: "startup SaaS" - max_priority=2
Job 3: "agence web" - max_priority=2
Job 4: "Ã©diteur logiciel" - max_priority=2
Job 5: "entreprise technologique" - max_priority=3
```

Total : ~250k leads, ~$150, ~6 heures

### Cas 3 : Ciblage GÃ©ographique

**Objectif** : Leads en Wallonie uniquement

```http
POST /api/v2/scraping/start
?query=bureau+comptable
&country=Wallonie
&max_priority=3
```

â†’ ~100 villes, ~$5, ~15 minutes, ~1500 leads

---

## ğŸ‰ RÃ©sumÃ©

**Tu dis simplement** :
```
"Scrape 'agence marketing' dans les villes moyennes"
```

**Le systÃ¨me fait** :
1. âœ… Charge les 36k villes depuis les CSV
2. âœ… Filtre avec stratÃ©gie intelligente (900 villes >20k hab)
3. âœ… Scrape 5 pages pour les grandes villes, 2 pour les petites
4. âœ… Sauvegarde au fur et Ã  mesure en Supabase
5. âœ… S'arrÃªte automatiquement quand plus de rÃ©sultats (Ã©conomise l'API)
6. âœ… Te donne la progression en temps rÃ©el
7. âœ… Peut reprendre si interruption

**Tu rÃ©cupÃ¨res** :
- 67 500 leads dÃ©dupliquÃ©s
- CoÃ»t : $34
- Temps : 90 minutes
- StockÃ©s en Supabase, prÃªts pour l'email gen !

---

## ğŸ“ Commandes Utiles

```bash
# Voir les logs en temps rÃ©el
docker logs -f kaleads-atomic-agents

# Voir uniquement les logs de scraping
docker logs -f kaleads-atomic-agents 2>&1 | grep "Scraping"

# RedÃ©marrer l'API
docker-compose restart

# Rebuild complet
cd /opt/kaleads-api && git pull && docker-compose up -d --build
```

**Bon scraping intelligent ! ğŸš€**
